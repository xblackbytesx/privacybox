name: llamacpp-coder

volumes:
  llamacpp-coder-models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DOCKER_ROOT}/llamacpp/models

services:
  llamacpp-coder:
    container_name: llamacpp-coder
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    security_opt:
      - no-new-privileges:true
      - label:disable
    ipc: host
    shm_size: 4g
    volumes:
      - llamacpp-coder-models:/models
      - /etc/localtime:/etc/localtime:ro
    devices:
      - /dev/dri:/dev/dri
    # environment:
    #   - VK_LOADER_DEBUG=error
    ports:
      - "11436:8080"
    # # Smarter but slower model
    # command: [
    #     "-m",
    #     "/models/Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf",
    #     "--host",
    #     "0.0.0.0",
    #     "--port",
    #     "8080",
    #     "-ngl",
    #     "40",
    #     # Unload model after 300 seconds (5 minutes) of inactivity
    #     "--timeout",
    #     "300",
    #     "-c",
    #     "8192",
    #     "-b",
    #     "512",
    #     # --no-mmap can help with stability on Vulkan
    #     "--no-mmap",
    #     "-fa",
    #     "off",
    #   ]
    # Slightly dumber but super fast model
    command: [
        "-m",
        "/models/Qwen2.5.1-Coder-7B-Instruct-Q6_K.gguf",
        "--host",
        "0.0.0.0",
        "--port",
        "8080",
        "-ngl",
        "99",
        # Unload model after 300 seconds (5 minutes) of inactivity
        "--timeout",
        "300",
        "-c",
        "16384",
        "-b",
        "2048",
        # --no-mmap can help with stability on Vulkan
        "--no-mmap",
        "-fa",
        "on",
      ]
    deploy:
      resources:
        limits:
          cpus: "10.0"
          memory: 32G
    restart: unless-stopped
